import os
from googleapiclient.discovery import build
import yt_dlp
import whisper
import isodate
import logging
import torch
from datetime import datetime
import threading
import db.db_adapter as db
from multiprocessing import Pool, cpu_count
from pathlib import Path
from apscheduler.schedulers.background import BackgroundScheduler
import time
import argparse
import tiktok_whisper_latest as tiktok
import random
from multiprocessing import Process

# ===== C·∫§U H√åNH =====
API_KEY = ""   # üîë thay b·∫±ng API key YouTube Data v3
CHANNEL_ID = "UCeCUvXYMbKs0BjOIRaIdDjw"  # channelId k√™nh mu·ªën l·∫•y
AUDIO_DIR = "downloads/audio"
SCRIPT_DIR = "downloads/transcripts"
LOG_DIR = "logs"
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(SCRIPT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ===== C·∫•u h√¨nh logging =====
log_file = os.path.join(LOG_DIR, "log.txt")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ]
)


# ===== B1: L·∫•y video m·ªõi nh·∫•t t·ª´ k√™nh =====
def get_latest_video(channel_id):
    youtube = build("youtube", "v3", developerKey=API_KEY)

    # L·∫•y activity m·ªõi nh·∫•t (t·ªën 1 unit)
    request = youtube.activities().list(
        part="snippet,contentDetails",
        channelId=channel_id,
        maxResults=5
    )
    response = request.execute()
    videos = []

    for item in response.get("items", []):
        # Ch·ªâ l·∫•y activity d·∫°ng upload video
        if "upload" not in item.get("contentDetails", {}):
            continue

        video_id = item["contentDetails"]["upload"]["videoId"]

        # G·ªçi videos().list ƒë·ªÉ l·∫•y chi ti·∫øt video
        video_request = youtube.videos().list(
            part="contentDetails,snippet,status",
            id=video_id
        )
        video_response = video_request.execute()

        if not video_response["items"]:
            continue

        details = video_response["items"][0]
        duration = details["contentDetails"]["duration"]
        seconds = isodate.parse_duration(duration).total_seconds()

        # Ki·ªÉm tra tr·∫°ng th√°i upload + quy·ªÅn ri√™ng t∆∞
        upload_status = details["status"].get("uploadStatus", "unknown")
        privacy_status = details["status"].get("privacyStatus", "unknown")

        if seconds > 60 and upload_status == "processed" and privacy_status == "public":
            published_at_str = details['snippet']['publishedAt']
            published_at_dt = datetime.strptime(published_at_str, "%Y-%m-%dT%H:%M:%SZ")

            logging.info(
                f"‚úÖ Video h·ª£p l·ªá: {details['snippet']['title']} "
                f"({seconds} gi√¢y), ƒëƒÉng l√∫c {published_at_dt}"
            )

            videos.append({
                "video_id": video_id,
                "title": details["snippet"]["title"],
                "url": f"https://www.youtube.com/watch?v={video_id}",
                "published_at": f"{published_at_dt}"
            })

    # Tr·∫£ v·ªÅ video m·ªõi nh·∫•t
    return videos[0] if videos else None


# ===== B2: T·∫£i audio (MP3) b·∫±ng yt-dlp =====
# def download_audio(video_url, dir, video_id):
#     ydl_opts = {
#         'format': 'bestaudio/best',
#         'outtmpl': os.path.join(dir, '%(title)s.%(ext)s'),
#         'postprocessors': [{
#             'key': 'FFmpegExtractAudio',
#             'preferredcodec': 'mp3',
#             'preferredquality': '192',
#         }],
#     }
#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:
#         info = ydl.extract_info(video_url, download=True)
#         filename = os.path.join(dir, f"{video_id}.mp3")
#         # ƒê·ªïi t√™n file ƒë√£ t·∫£i th√†nh video_id.mp3
#         downloaded_file = os.path.splitext(ydl.prepare_filename(info))[0] + ".mp3"
#         if downloaded_file != filename:
#             os.rename(downloaded_file, filename)
#         return filename
def download_audio(video_url, dir, video_id):
    # ƒê∆∞·ªùng d·∫´n file ƒë√≠ch (lu√¥n .mp3)
    output_path = os.path.join(dir, f"{video_id}.%(ext)s")

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': output_path,
        'quiet': True,
        'no_warnings': True,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(video_url, download=True)

    # Sau postprocess, file ch·∫Øc ch·∫Øn l√† .mp3
    final_file = os.path.join(dir, f"{video_id}.mp3")
    return final_file

def transcribe_audio(file_path):
    logging.info("‚è≥ ƒêang load m√¥ h√¨nh Whisper...")
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    logging.info(f"üñ•Ô∏è ƒêang ch·∫°y tr√™n: {device.upper()}")
    logging.info(torch.cuda.get_device_name(torch.device(device)))
    model = whisper.load_model("large").to(device)
    logging.info("üéôÔ∏è ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i...")
    
    result = model.transcribe(file_path, language="vi")
    return result["text"]

# def process_videos_from_db():
#     logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
#     try:
#         conn = db.get_connection()
#         if conn is None:
#             print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu PostgreSQL.")
#             exit(1)

#         try:
#             with conn.cursor() as cursor:
#                 cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
#                 rows = cursor.fetchall()
#                 data_list = list(rows)
#         except Exception as e:
#             print("‚ùå L·ªói khi truy v·∫•n:", e)
#             exit(1)
#         finally:
#             conn.close()

#         for id, link, name in data_list:
#             print(f"üîó ƒêang truy c·∫≠p nh√≥m: {name}_{link}")
#             latest = get_latest_video(link)
#             if latest:
#                 logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")
#                 logging.info(f"üîó Link: {latest['url']}")
#                 if db.validate_yt_post(latest['title'], latest['url']):
#                     audio_file = download_and_save_audio(latest['url'], AUDIO_DIR)
#                     if audio_file:
#                         thread = threading.Thread(target=process_audio_file, args=(id, latest, audio_file))
#                         thread.start()
#                 else:
#                     logging.error("‚ùå ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")
#             else:
#                 logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")
#     except Exception as e:
#         logging.error(f"‚ùå L·ªói: {str(e)}", exc_info=True)

# def process_videos_from_db():
#     logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
#     try:
#         conn = db.get_connection()
#         if conn is None:
#             print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu PostgreSQL.")
#             exit(1)

#         try:
#             with conn.cursor() as cursor:
#                 cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
#                 rows = cursor.fetchall()
#                 data_list = list(rows)
#         except Exception as e:
#             print("‚ùå L·ªói khi truy v·∫•n:", e)
#             exit(1)
#         finally:
#             conn.close()

#         # chu·∫©n b·ªã danh s√°ch task
#         tasks = []
#         for id, link, name in data_list:
#             print(f"üîó ƒêang truy c·∫≠p nh√≥m: {name}_{link}")
#             latest = get_latest_video(link)
#             if latest:
#                 logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")
#                 logging.info(f"üîó Link: {latest['url']}")
#                 if db.validate_yt_post(latest['title'], latest['url']):
#                     audio_file = download_and_save_audio(latest['url'], AUDIO_DIR)
#                     if audio_file:
#                         tasks.append((id, latest, audio_file))
#                 else:
#                     logging.error("‚ùå ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")
#             else:
#                 logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")

#         # ch·∫°y song song b·∫±ng multiprocessing
#         if tasks:
#             max_workers = min(len(tasks), cpu_count())  # gi·ªõi h·∫°n theo s·ªë CPU
#             with Pool(processes=max_workers) as pool:
#                 pool.starmap(process_audio_file, tasks)  # truy·ªÅn nhi·ªÅu arg

#     except Exception as e:
#         logging.error(f"‚ùå L·ªói: {str(e)}", exc_info=True)

def download_and_save_audio(video_url, audio_dir, video_id):
    logging.info("‚úÖ Video ch∆∞a t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu, b·∫Øt ƒë·∫ßu t·∫£i audio...")
    logging.info("‚¨áÔ∏è ƒêang t·∫£i audio (mp3)...")
    try:
        audio_file = download_audio(video_url, audio_dir, video_id)
        logging.info(f"‚úÖ ƒê√£ t·∫£i: {audio_file}")
        return audio_file
    except Exception as e:
        logging.error(f"‚ùå L·ªói khi t·∫£i audio: {e}")
        return None

def process_audio_file(id, audio_path):
    logging.info(f"üîä ƒêang x·ª≠ l√Ω audio: {audio_path}")
    audio_file = str(audio_path)
    if not os.path.exists(audio_file):
        logging.error(f"‚ùå File audio kh√¥ng t·ªìn t·∫°i: {audio_file}")
        return
    transcript = transcribe_audio(audio_file)
    try:
        os.remove(audio_file)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a file audio: {audio_file}. L·ªói: {e}")

    logging.info("===== N·ªòI DUNG VIDEO =====")
    logging.info(transcript)

    if transcript.strip() == "":
        logging.error("‚ùå Kh√¥ng th·ªÉ nh·∫≠n di·ªán n·ªôi dung video.")
        return

    if db.update_yt_post_content(id, transcript):
        logging.info("‚úÖ ƒê√£ c·∫≠p nh·∫≠t n·ªôi dung video v√†o c∆° s·ªü d·ªØ li·ªáu.")

    # if db.insert_yt_post(f"{id}_{latest['video_id']}", latest['title'], latest['url'], transcript, latest['published_at']):
    #     logging.info("‚úÖ ƒê√£ l∆∞u n·ªôi dung video v√†o c∆° s·ªü d·ªØ li·ªáu.")
    #     base_txt_path = os.path.join(SCRIPT_DIR, os.path.splitext(os.path.basename(audio_file))[0] + ".txt")
    #     txt_path = base_txt_path
    #     count = 1
    #     while os.path.exists(txt_path):
    #         txt_path = os.path.join(SCRIPT_DIR, os.path.splitext(os.path.basename(audio_file))[0] + f"_{count}.txt")
    #         count += 1
    #     with open(txt_path, "w", encoding="utf-8") as f:
    #         f.write(transcript)
    #     logging.info(f"üíæ ƒê√£ l∆∞u transcript v√†o: {txt_path}")
    # else:
    #     logging.error("‚ùå N·ªôi dung video ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")

def fetch_and_download_audio():
    logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
    conn = db.get_connection()
    if conn is None:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
        return

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
            rows = cursor.fetchall()
            data_list = list(rows)
    finally:
        conn.close()

    for id, link, name in data_list:
        logging.info(f"üîó Nh√≥m: {name}_{link}")
        latest = get_latest_video(link)
        if not latest:
            logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")
            continue

        logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")

        if not db.validate_yt_post(latest['title'], latest['url']):
            logging.warning("‚ö†Ô∏è ƒê√£ t·ªìn t·∫°i trong DB, b·ªè qua.")
            continue
        video_id = f"{id}_{latest['video_id']}"
        audio_file = download_and_save_audio(latest['url'], AUDIO_DIR, video_id)
        if audio_file:
            logging.info(f"üíæ ƒê√£ l∆∞u audio: {audio_file}")
            # Ghi metadata v√†o DB ƒë·ªÉ Job 2 x·ª≠ l√Ω
            if db.insert_yt_post(video_id, latest['title'], latest['url'], "", latest['published_at']):
                logging.info("‚úÖ ƒê√£ l∆∞u metadata video v√†o c∆° s·ªü d·ªØ li·ªáu.")
        time.sleep(random.randint(5,15))
    # Sau khi xong th√¨ add Job get last tiktok video ngay
    print("‚û°Ô∏è Job1 ho√†n t·∫•t, th√™m Job get last tiktok video v√†o l·ªãch")
    scheduler.add_job(
        fectch_download_audio_tiktok,
        "date",   # ch·ªâ ch·∫°y 1 l·∫ßn
        run_date=datetime.now(),  # ch·∫°y ngay l·∫≠p t·ª©c
        id="job3_once",
        replace_existing=True
    )


# def process_audio_job():
#     logging.info("üé∂ ƒêang t√¨m audio ch∆∞a x·ª≠ l√Ω...")
#     conn = db.get_connection()
#     if conn is None:
#         logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
#         return

#     try:
#         with conn.cursor() as cursor:
#             cursor.execute("SELECT post_id FROM yt_post WHERE post_processed = false")
#             rows = cursor.fetchall()
#     finally:
#         conn.close()

#     if not rows:
#         logging.info("‚úÖ Kh√¥ng c√≥ audio m·ªõi ƒë·ªÉ x·ª≠ l√Ω.")
#         return

#     for row in rows:
#         post_id = row[0]
#         audio_file = os.path.join(AUDIO_DIR, f"{post_id}.mp3")
#         process_audio_file(post_id, Path(audio_file))

    # tasks = [(post_id, Path(filename = os.path.join(AUDIO_DIR, f"{post_id}.mp3"))) for post_id in rows]

    # if not tasks:
    #     logging.info("‚úÖ Kh√¥ng c√≥ audio m·ªõi.")
    #     return

    # max_workers = min(len(tasks), cpu_count())
    # with Pool(processes=max_workers) as pool:
    #     pool.starmap(process_audio_file, tasks)

    # # Sau khi x·ª≠ l√Ω xong, update tr·∫°ng th√°i
    # conn = db.get_connection()
    # with conn.cursor() as cursor:
    #     for yt_id, yt_video_id, _ in tasks:
    #         cursor.execute("UPDATE pending_audio SET processed = true WHERE yt_id = %s AND yt_video_id = %s", (yt_id, yt_video_id))
    #     conn.commit()
    # conn.close()

def worker(gpu_id, tasks):
    # G√°n GPU cho process n√†y
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    # Load model Whisper 1 l·∫ßn tr√™n GPU ƒë√≥
    logging.info(f"[GPU {gpu_id}] Load model Whisper...")
    model = whisper.load_model("large").to("cuda")

    for post_id, audio_file in tasks:
        logging.info(f"[GPU {gpu_id}] ƒêang x·ª≠ l√Ω {audio_file}")
        try:
            result = model.transcribe(str(audio_file), language="vi")
            text = result["text"]

            # L∆∞u k·∫øt qu·∫£ ra file txt (ho·∫∑c update DB)
            # out_file = Path(audio_file).with_suffix(".txt")
            # with open(out_file, "w", encoding="utf-8") as f:
            #     f.write(text)
            if text.strip() == "":
                logging.error("‚ùå Kh√¥ng th·ªÉ nh·∫≠n di·ªán n·ªôi dung video.")
                return

            if db.update_yt_post_content(post_id, text):
                logging.info(f"[GPU {gpu_id}] ‚úÖ Xong {post_id}")
            
            try:
                os.remove(audio_file)
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a file audio: {audio_file}. L·ªói: {e}")

            
        except Exception as e:
            logging.error(f"[GPU {gpu_id}] ‚ùå L·ªói {post_id}: {e}")

def process_audio_job():
    logging.info("üé∂ ƒêang t√¨m audio ch∆∞a x·ª≠ l√Ω...")

    conn = db.get_connection()
    if conn is None:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
        return

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT post_id FROM yt_post WHERE post_processed = false")
            rows = cursor.fetchall()
    finally:
        conn.close()

    if not rows:
        logging.info("‚úÖ Kh√¥ng c√≥ audio m·ªõi ƒë·ªÉ x·ª≠ l√Ω.")
        return

    # Chu·∫©n b·ªã list (post_id, file_path)
    tasks = []
    for row in rows:
        post_id = row[0]
        audio_file = os.path.join(AUDIO_DIR, f"{post_id}.mp3")
        tasks.append((post_id, Path(audio_file)))

    # Chia xen k·∫Ω cho 2 GPU
    tasks_gpu0 = tasks[0::2]
    tasks_gpu1 = tasks[1::2]

    # T·∫°o 2 process song song
    p0 = Process(target=worker, args=(0, tasks_gpu0))
    p1 = Process(target=worker, args=(1, tasks_gpu1))

    p0.start()
    p1.start()
    p0.join()
    p1.join()

    logging.info("üéâ Ho√†n t·∫•t x·ª≠ l√Ω t·∫•t c·∫£ audio ch∆∞a x·ª≠ l√Ω.")

def fectch_download_audio_tiktok():

    logging.info("üîç ƒêang t√¨m tiktok video m·ªõi nh·∫•t...")
    conn = db.get_connection()
    if conn is None:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
        return

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT tt_id, tt_link, tt_name FROM tt_group ORDER BY RANDOM()")
            rows = cursor.fetchall()
            data_list = list(rows)
    finally:
        conn.close()

    for id, link, name in data_list:
        logging.info(f"üîó Nh√≥m: {id}/{name}_{link}")

        user_name = link  # Thay th·∫ø b·∫±ng t√™n k√™nh TikTok b·∫°n mu·ªën
        cookies_file = None  # Thay th·∫ø b·∫±ng ƒë∆∞·ªùng d·∫´n cookies.txt n·∫øu c·∫ßn
        model_size = "medium"  # K√≠ch th∆∞·ªõc m√¥ h√¨nh Whisper
        language = 'vi'  # Ng√¥n ng·ªØ, ƒë·ªÉ tr·ªëng ƒë·ªÉ t·ª± ƒë·ªông    
        outdir = "downloads/audio"  # Th∆∞ m·ª•c t·∫£i audio
        transdir = "downloads/transcripts"  # Th∆∞ m·ª•c l∆∞u transcript
        args = argparse.Namespace(
            username=user_name,
            cookies=cookies_file,
            model=model_size,
            lang=language,
            outdir=outdir,
            transdir=transdir
        )

        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] L·∫•y video m·ªõi nh·∫•t c·ªßa @{args.username} ...")
        latest_entry = tiktok.get_latest_tiktok_video_entry(args.username)

        # C·ªë g·∫Øng l·∫•y URL video.
        video_url = latest_entry.get("url") or latest_entry.get("webpage_url") or latest_entry.get("original_url")
        if not video_url:
            # Trong extract_flat, TikTok ƒë√¥i khi tr·∫£ v·ªÅ 'id' thay v√¨ URL ƒë·∫ßy ƒë·ªß
            vid_id = latest_entry.get("id")
            if vid_id:
                video_url = f"https://www.tiktok.com/@{args.username}/video/{vid_id}"
            else:
                raise RuntimeError("Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c URL video m·ªõi nh·∫•t.")

        ts = latest_entry.get("timestamp")
        ts_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S") if ts else "N/A"
        logging.info(f"Video m·ªõi nh·∫•t: {video_url}\nTh·ªùi gian ƒëƒÉng: {ts_str}")

        if not db.validate_yt_post(latest_entry['title'], video_url):
            logging.warning("‚ö†Ô∏è ƒê√£ t·ªìn t·∫°i trong DB, b·ªè qua.")
            continue

        logging.info("\n[T·∫£i audio b·∫±ng yt-dlp] ...")
        video_id = f"t_{args.username}_{video_url.rstrip('/').split('/')[-1]}"
        audio_path = tiktok.download_best_audio(video_url, outdir=args.outdir, vid_id=video_id)
        if audio_path:
            logging.info(f"üíæ ƒê√£ l∆∞u audio: {audio_path}")
            # Ghi metadata v√†o DB ƒë·ªÉ Job 2 x·ª≠ l√Ω
            if db.insert_yt_post(video_id, latest_entry['title'], video_url, "", ts_str):
                logging.info("‚úÖ ƒê√£ l∆∞u metadata video v√†o c∆° s·ªü d·ªØ li·ªáu.")
        time.sleep(random.randint(5,15))

                   
if __name__ == "__main__":
    # process_audio_job()  # L·∫•y video m·ªõi nh·∫•t v√† t·∫£i audio
    scheduler = BackgroundScheduler()

    # Job 1: m·ªói 4 gi·ªù (0, 4, 8, 12, 16, 20)
    scheduler.add_job(
        fetch_and_download_audio,
        "cron",
        hour="0,4,6,8,18,20,22",
        id="fetch_job",
        max_instances=1,
        coalesce=True,
        misfire_grace_time=1,
        next_run_time=datetime.now() #cho ch·∫°y lu√¥n
    )
    # Job 2: m·ªói gi·ªù
    scheduler.add_job(
        process_audio_job,
        "interval",
        hours=1,
        id="process_job",
        max_instances=1,        # ch·ªâ cho ph√©p 1 job ch·∫°y
        coalesce=True,          # kh√¥ng ch·∫°y b√π n·∫øu l·ª°
        misfire_grace_time=1,    # n·∫øu l·ª° gi·ªù th√¨ b·ªè qua
        # next_run_time=datetime.now() #cho ch·∫°y lu√¥n
    )
    # Start scheduler
    scheduler.start()

    print("‚úÖ Scheduler ƒë√£ kh·ªüi ƒë·ªông. Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng.")

    try:
        while True:
            time.sleep(1)
    except (KeyboardInterrupt, SystemExit):
        scheduler.shutdown()
        print("üõë Scheduler ƒë√£ d·ª´ng.")
# pyinstaller --onefile --add-data "C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\whisper\assets;whisper/assets" gg_api.py
# pyinstaller --onefile --exclude-module torch --exclude-module torchvision --exclude-module torchaudio --add-data "C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\whisper\assets;whisper/assets" gg_api.py

