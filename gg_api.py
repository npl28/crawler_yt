import os
from googleapiclient.discovery import build
import yt_dlp
import whisper
import isodate
import logging
import torch
from datetime import datetime
import threading
import db.db_adapter as db
from multiprocessing import Pool, cpu_count
from pathlib import Path

# ===== C·∫§U H√åNH =====
API_KEY = ""   # üîë thay b·∫±ng API key YouTube Data v3
CHANNEL_ID = "UCeCUvXYMbKs0BjOIRaIdDjw"  # channelId k√™nh mu·ªën l·∫•y
AUDIO_DIR = "downloads/audio"
SCRIPT_DIR = "downloads/transcripts"
LOG_DIR = "logs"
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(SCRIPT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ===== C·∫•u h√¨nh logging =====
log_file = os.path.join(LOG_DIR, "log.txt")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ]
)


# ===== B1: L·∫•y video m·ªõi nh·∫•t t·ª´ k√™nh =====
def get_latest_video(channel_id):
    youtube = build("youtube", "v3", developerKey=API_KEY)
    
    # Ch·ªâ ƒë∆∞·ª£c d√πng snippet ·ªü ƒë√¢y
    request = youtube.search().list(
        part="snippet",
        channelId=channel_id,
        order="date",
        maxResults=5,
        type="video",
        videoDuration="any"
    )
    response = request.execute()
    videos = []

    for item in response.get("items", []):
        video_id = item["id"]["videoId"]

        # G·ªçi videos().list ƒë·ªÉ l·∫•y contentDetails
        video_request = youtube.videos().list(
            part="contentDetails,snippet,status",
            id=video_id
        )
        video_response = video_request.execute()

        if video_response["items"]:
            details = video_response["items"][0]
            duration = details["contentDetails"]["duration"]
            seconds = isodate.parse_duration(duration).total_seconds()

            # ki·ªÉm tra video ƒë√£ publish hay ch∆∞a
            upload_status = details["status"].get("uploadStatus", "unknown")
            privacy_status = details["status"].get("privacyStatus", "unknown")

            if seconds > 60 and upload_status == "processed" and privacy_status == "public":
                published_at_str = details['snippet']['publishedAt']
                published_at_dt = datetime.strptime(published_at_str, "%Y-%m-%dT%H:%M:%SZ")
                logging.info(f"‚úÖ Video h·ª£p l·ªá: {item['snippet']['title']} ({seconds} gi√¢y), ƒëƒÉng l√∫c {published_at_dt}")
                videos.append({
                    "video_id": video_id,
                    "title": details["snippet"]["title"],
                    "url": f"https://www.youtube.com/watch?v={video_id}",
                    "published_at": f"{published_at_dt}"
                })

    return videos[0] if videos else None


# ===== B2: T·∫£i audio (MP3) b·∫±ng yt-dlp =====
# def download_audio(video_url, dir, video_id):
#     ydl_opts = {
#         'format': 'bestaudio/best',
#         'outtmpl': os.path.join(dir, '%(title)s.%(ext)s'),
#         'postprocessors': [{
#             'key': 'FFmpegExtractAudio',
#             'preferredcodec': 'mp3',
#             'preferredquality': '192',
#         }],
#     }
#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:
#         info = ydl.extract_info(video_url, download=True)
#         filename = os.path.join(dir, f"{video_id}.mp3")
#         # ƒê·ªïi t√™n file ƒë√£ t·∫£i th√†nh video_id.mp3
#         downloaded_file = os.path.splitext(ydl.prepare_filename(info))[0] + ".mp3"
#         if downloaded_file != filename:
#             os.rename(downloaded_file, filename)
#         return filename
def download_audio(video_url, dir, video_id):
    # ƒê∆∞·ªùng d·∫´n file ƒë√≠ch (lu√¥n .mp3)
    output_path = os.path.join(dir, f"{video_id}.%(ext)s")

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': output_path,
        'quiet': True,
        'no_warnings': True,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(video_url, download=True)

    # Sau postprocess, file ch·∫Øc ch·∫Øn l√† .mp3
    final_file = os.path.join(dir, f"{video_id}.mp3")
    return final_file

def transcribe_audio(file_path):
    logging.info("‚è≥ ƒêang load m√¥ h√¨nh Whisper...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"üñ•Ô∏è ƒêang ch·∫°y tr√™n: {device.upper()}")
    
    model = whisper.load_model("medium").to(device)
    logging.info("üéôÔ∏è ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i...")
    
    result = model.transcribe(file_path, language="vi")
    return result["text"]

# def process_videos_from_db():
#     logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
#     try:
#         conn = db.get_connection()
#         if conn is None:
#             print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu PostgreSQL.")
#             exit(1)

#         try:
#             with conn.cursor() as cursor:
#                 cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
#                 rows = cursor.fetchall()
#                 data_list = list(rows)
#         except Exception as e:
#             print("‚ùå L·ªói khi truy v·∫•n:", e)
#             exit(1)
#         finally:
#             conn.close()

#         for id, link, name in data_list:
#             print(f"üîó ƒêang truy c·∫≠p nh√≥m: {name}_{link}")
#             latest = get_latest_video(link)
#             if latest:
#                 logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")
#                 logging.info(f"üîó Link: {latest['url']}")
#                 if db.validate_yt_post(latest['title'], latest['url']):
#                     audio_file = download_and_save_audio(latest['url'], AUDIO_DIR)
#                     if audio_file:
#                         thread = threading.Thread(target=process_audio_file, args=(id, latest, audio_file))
#                         thread.start()
#                 else:
#                     logging.error("‚ùå ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")
#             else:
#                 logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")
#     except Exception as e:
#         logging.error(f"‚ùå L·ªói: {str(e)}", exc_info=True)

# def process_videos_from_db():
#     logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
#     try:
#         conn = db.get_connection()
#         if conn is None:
#             print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu PostgreSQL.")
#             exit(1)

#         try:
#             with conn.cursor() as cursor:
#                 cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
#                 rows = cursor.fetchall()
#                 data_list = list(rows)
#         except Exception as e:
#             print("‚ùå L·ªói khi truy v·∫•n:", e)
#             exit(1)
#         finally:
#             conn.close()

#         # chu·∫©n b·ªã danh s√°ch task
#         tasks = []
#         for id, link, name in data_list:
#             print(f"üîó ƒêang truy c·∫≠p nh√≥m: {name}_{link}")
#             latest = get_latest_video(link)
#             if latest:
#                 logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")
#                 logging.info(f"üîó Link: {latest['url']}")
#                 if db.validate_yt_post(latest['title'], latest['url']):
#                     audio_file = download_and_save_audio(latest['url'], AUDIO_DIR)
#                     if audio_file:
#                         tasks.append((id, latest, audio_file))
#                 else:
#                     logging.error("‚ùå ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")
#             else:
#                 logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")

#         # ch·∫°y song song b·∫±ng multiprocessing
#         if tasks:
#             max_workers = min(len(tasks), cpu_count())  # gi·ªõi h·∫°n theo s·ªë CPU
#             with Pool(processes=max_workers) as pool:
#                 pool.starmap(process_audio_file, tasks)  # truy·ªÅn nhi·ªÅu arg

#     except Exception as e:
#         logging.error(f"‚ùå L·ªói: {str(e)}", exc_info=True)

def download_and_save_audio(video_url, audio_dir, video_id):
    logging.info("‚úÖ Video ch∆∞a t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu, b·∫Øt ƒë·∫ßu t·∫£i audio...")
    logging.info("‚¨áÔ∏è ƒêang t·∫£i audio (mp3)...")
    try:
        audio_file = download_audio(video_url, audio_dir, video_id)
        logging.info(f"‚úÖ ƒê√£ t·∫£i: {audio_file}")
        return audio_file
    except Exception as e:
        logging.error(f"‚ùå L·ªói khi t·∫£i audio: {e}")
        return None

def process_audio_file(id, audio_path):
    logging.info(f"üîä ƒêang x·ª≠ l√Ω audio: {audio_path}")
    audio_file = str(audio_path)
    if not os.path.exists(audio_file):
        logging.error(f"‚ùå File audio kh√¥ng t·ªìn t·∫°i: {audio_file}")
        return
    transcript = transcribe_audio(audio_file)
    try:
        os.remove(audio_file)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a file audio: {audio_file}. L·ªói: {e}")

    logging.info("===== N·ªòI DUNG VIDEO =====")
    logging.info(transcript)

    if transcript.strip() == "":
        logging.error("‚ùå Kh√¥ng th·ªÉ nh·∫≠n di·ªán n·ªôi dung video.")
        return

    if db.update_yt_post_content(id, transcript):
        logging.info("‚úÖ ƒê√£ c·∫≠p nh·∫≠t n·ªôi dung video v√†o c∆° s·ªü d·ªØ li·ªáu.")

    # if db.insert_yt_post(f"{id}_{latest['video_id']}", latest['title'], latest['url'], transcript, latest['published_at']):
    #     logging.info("‚úÖ ƒê√£ l∆∞u n·ªôi dung video v√†o c∆° s·ªü d·ªØ li·ªáu.")
    #     base_txt_path = os.path.join(SCRIPT_DIR, os.path.splitext(os.path.basename(audio_file))[0] + ".txt")
    #     txt_path = base_txt_path
    #     count = 1
    #     while os.path.exists(txt_path):
    #         txt_path = os.path.join(SCRIPT_DIR, os.path.splitext(os.path.basename(audio_file))[0] + f"_{count}.txt")
    #         count += 1
    #     with open(txt_path, "w", encoding="utf-8") as f:
    #         f.write(transcript)
    #     logging.info(f"üíæ ƒê√£ l∆∞u transcript v√†o: {txt_path}")
    # else:
    #     logging.error("‚ùå N·ªôi dung video ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.")

def fetch_and_download_audio():
    logging.info("üîç ƒêang t√¨m video m·ªõi nh·∫•t...")
    conn = db.get_connection()
    if conn is None:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
        return

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT yt_id, yt_link, yt_name FROM yt_group ORDER BY RANDOM()")
            rows = cursor.fetchall()
            data_list = list(rows)
    finally:
        conn.close()

    for id, link, name in data_list:
        logging.info(f"üîó Nh√≥m: {name}_{link}")
        latest = get_latest_video(link)
        if not latest:
            logging.warning("‚ùå Kh√¥ng t√¨m th·∫•y video.")
            continue

        logging.info(f"üì∫ Video m·ªõi nh·∫•t {latest['video_id']}: {latest['title']}")

        if not db.validate_yt_post(latest['title'], latest['url']):
            logging.warning("‚ö†Ô∏è ƒê√£ t·ªìn t·∫°i trong DB, b·ªè qua.")
            continue
        video_id = f"{id}_{latest['video_id']}"
        audio_file = download_and_save_audio(latest['url'], AUDIO_DIR, video_id)
        if audio_file:
            logging.info(f"üíæ ƒê√£ l∆∞u audio: {audio_file}")
            # Ghi metadata v√†o DB ƒë·ªÉ Job 2 x·ª≠ l√Ω
            if db.insert_yt_post(video_id, latest['title'], latest['url'], "", latest['published_at']):
                logging.info("‚úÖ ƒê√£ l∆∞u metadata video v√†o c∆° s·ªü d·ªØ li·ªáu.")


def process_audio_job():
    logging.info("üé∂ ƒêang t√¨m audio ch∆∞a x·ª≠ l√Ω...")
    conn = db.get_connection()
    if conn is None:
        logging.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB")
        return

    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT post_id FROM yt_post WHERE post_processed = false")
            rows = cursor.fetchall()
    finally:
        conn.close()

    tasks = [(post_id, Path(filename = os.path.join(AUDIO_DIR, f"{post_id}.mp3"))) for post_id in rows]

    if not tasks:
        logging.info("‚úÖ Kh√¥ng c√≥ audio m·ªõi.")
        return

    max_workers = min(len(tasks), cpu_count())
    with Pool(processes=max_workers) as pool:
        pool.starmap(process_audio_file, tasks)

    # # Sau khi x·ª≠ l√Ω xong, update tr·∫°ng th√°i
    # conn = db.get_connection()
    # with conn.cursor() as cursor:
    #     for yt_id, yt_video_id, _ in tasks:
    #         cursor.execute("UPDATE pending_audio SET processed = true WHERE yt_id = %s AND yt_video_id = %s", (yt_id, yt_video_id))
    #     conn.commit()
    # conn.close()
                   
if __name__ == "__main__":
    fetch_and_download_audio()
# pyinstaller --onefile --add-data "C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\whisper\assets;whisper/assets" gg_api.py
# pyinstaller --onefile --exclude-module torch --exclude-module torchvision --exclude-module torchaudio --add-data "C:\Users\PC\AppData\Local\Programs\Python\Python311\Lib\site-packages\whisper\assets;whisper/assets" gg_api.py

